{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting line_profiler\n",
      "  Downloading line_profiler-4.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Downloading line_profiler-4.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (714 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m714.8/714.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: line_profiler\n",
      "Successfully installed line_profiler-4.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     40\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;129m@profile\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_code\u001b[39m():\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'profile' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import line_profiler\n",
    "%load_ext line_profiler\n",
    "corpus = [ \"The king ruled the kingdom with wisdom.\", \"The queen ruled the kingdom with grace.\", \"A wise man gave counsel to the king.\", \"A wise woman gave counsel to the queen.\", \"The king and the queen hosted a grand ball.\", \"The man went to the market.\", \"The woman went to the garden.\", \"The king and the man discussed politics.\", \"The queen and the woman discussed art.\", \"The king and the woman went for a walk.\", \"The man and the queen had a conversation.\", \"The wise king made important decisions.\", \"The wise woman offered valuable advice.\", \"The king and the man played chess.\", \"The queen and the woman painted together.\" ]\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'the quick brown fox jumped over the lazy dog',\n",
    "    'the brown fox is quick and the blue dog is lazy',\n",
    "    'the quick blue dog jumped over the lazy brown fox'\n",
    "]\n",
    "# Tokenize the sentences and build a vocabulary\n",
    "tokens = [sentence.split() for sentence in corpus]\n",
    "vocabulary = set(word for sentence in tokens for word in sentence)\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create one-hot encoded vectors for each word in the vocabulary\n",
    "vocab_size = len(vocabulary)\n",
    "one_hot_encodings = np.eye(vocab_size)\n",
    "\n",
    "# Define the context window size\n",
    "window_size = 2\n",
    "training_pairs = []\n",
    "\n",
    "# Generate training pairs with one-hot encoding\n",
    "for sentence in tokens:\n",
    "    sentence_indices = [word_to_index[word] for word in sentence]\n",
    "    for i, word_index in enumerate(sentence_indices):\n",
    "        for j in range(max(i - window_size, 0), min(i + window_size + 1, len(sentence_indices))):\n",
    "            if i != j:\n",
    "                center_word_vector = one_hot_encodings[word_index]  # One-hot encoded center word vector\n",
    "                context_word_vector = one_hot_encodings[sentence_indices[j]]  # One-hot encoded context word vector\n",
    "                training_pairs.append((center_word_vector, context_word_vector))\n",
    "\n",
    "# Define the Word2Vec model architecture (Skip-gram)\n",
    "embedding_size = 5  # Small embedding size for simplicity\n",
    "W = np.random.rand(vocab_size, embedding_size)  # Input to hidden layer weights (Vocab size x Embedding size)\n",
    "W_prime = np.random.rand(embedding_size, vocab_size)  # Hidden to output layer weights (Embedding size x Vocab size)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1\n",
    "\n",
    "@profile\n",
    "def training_code():\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for center_word_vector, context_word_vector in training_pairs:\n",
    "            # Forward pass\n",
    "            h = np.dot(center_word_vector, W)  # Hidden layer output (1 x Embedding size)\n",
    "            u = np.dot(h, W_prime)  # Score vector (1 x Vocab size)\n",
    "            y_pred = np.exp(u) / sum(np.exp(u))  # Softmax output (1 x Vocab size)\n",
    "            # Backward pass\n",
    "            EI = y_pred - context_word_vector  # Error indicator vector (1 x Vocab size)\n",
    "            dW_prime = np.outer(h, EI)  # Gradient for W_prime (Embedding size x Vocab size)\n",
    "            dW = np.outer(center_word_vector, np.dot( W_prime, EI))  # Gradient for W (Vocab size x Embedding size)\n",
    "            \n",
    "            # Update weights\n",
    "            W -= learning_rate * dW\n",
    "            W_prime -= learning_rate * dW_prime\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
