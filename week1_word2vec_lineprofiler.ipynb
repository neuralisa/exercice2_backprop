{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting line_profiler\n",
      "  Downloading line_profiler-4.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Downloading line_profiler-4.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (714 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m714.8/714.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: line_profiler\n",
      "Successfully installed line_profiler-4.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "Epoch 0, Loss: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0.100751 s\n",
      "File: /tmp/ipykernel_3716555/1219967510.py\n",
      "Function: training_code at line 39\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    39                                           def training_code():\n",
      "    40                                               # Define the Word2Vec model architecture (Skip-gram)\n",
      "    41         1        420.0    420.0      0.0      embedding_size = 10  # Small embedding size for simplicity\n",
      "    42         1      14681.0  14681.0      0.0      W = np.random.rand(vocab_size, embedding_size)  # Input to hidden layer weights (Vocab size x Embedding size)\n",
      "    43         1      26170.0  26170.0      0.0      W_prime = np.random.rand(embedding_size, vocab_size)  # Hidden to output layer weights (Embedding size x Vocab size)\n",
      "    44                                           \n",
      "    45                                               # Training parameters\n",
      "    46         1        380.0    380.0      0.0      learning_rate = 0.01\n",
      "    47         1        180.0    180.0      0.0      epochs = 50\n",
      "    48                                           \n",
      "    49                                           \n",
      "    50                                               # Training loop\n",
      "    51         1        220.0    220.0      0.0      losses = []\n",
      "    52        51      11600.0    227.5      0.0      for epoch in range(epochs):\n",
      "    53        50       5610.0    112.2      0.0          loss = 0\n",
      "    54      5150     876717.0    170.2      0.9          for center_word_vector, context_word_vector in training_pairs:\n",
      "    55                                                       # Forward pass\n",
      "    56      5100    8350095.0   1637.3      8.3              h = np.dot(center_word_vector, W)  # Hidden layer output (1 x Embedding size)\n",
      "    57      5100   13141213.0   2576.7     13.0              u = np.dot(h, W_prime)  # Score vector (1 x Vocab size)\n",
      "    58      5100   20150663.0   3951.1     20.0              y_pred = np.exp(u) / sum(np.exp(u))  # Softmax output (1 x Vocab size)\n",
      "    59                                                       # Backward pass\n",
      "    60      5100    3259752.0    639.2      3.2              EI = y_pred - context_word_vector  # Error indicator vector (1 x Vocab size)\n",
      "    61      5100   16010928.0   3139.4     15.9              dW_prime = np.outer(h, EI)  # Gradient for W_prime (Embedding size x Vocab size)\n",
      "    62      5100   22642777.0   4439.8     22.5              dW = np.outer(center_word_vector, np.dot( W_prime, EI))  # Gradient for W (Vocab size x Embedding size)\n",
      "    63                                                       \n",
      "    64                                                       # Update weights\n",
      "    65      5100    8762923.0   1718.2      8.7              W -= learning_rate * dW\n",
      "    66      5100    7442610.0   1459.3      7.4              W_prime -= learning_rate * dW_prime\n",
      "    67        50      20880.0    417.6      0.0          losses.append(loss)\n",
      "    68                                           \n",
      "    69                                           \n",
      "    70        50      21811.0    436.2      0.0          if epoch % 100 == 0:\n",
      "    71         1      11541.0  11541.0      0.0              print(f'Epoch {epoch}, Loss: {loss}')"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import line_profiler\n",
    "%load_ext line_profiler\n",
    "import line_profiler\n",
    "corpus = [ \"The king ruled the kingdom with wisdom.\", \"The queen ruled the kingdom with grace.\", \"A wise man gave counsel to the king.\", \"A wise woman gave counsel to the queen.\", \"The king and the queen hosted a grand ball.\", \"The man went to the market.\", \"The woman went to the garden.\", \"The king and the man discussed politics.\", \"The queen and the woman discussed art.\", \"The king and the woman went for a walk.\", \"The man and the queen had a conversation.\", \"The wise king made important decisions.\", \"The wise woman offered valuable advice.\", \"The king and the man played chess.\", \"The queen and the woman painted together.\" ]\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'the quick brown fox jumped over the lazy dog',\n",
    "    'the brown fox is quick and the blue dog is lazy',\n",
    "    'the quick blue dog jumped over the lazy brown fox'\n",
    "]\n",
    "# Tokenize the sentences and build a vocabulary\n",
    "tokens = [sentence.split() for sentence in corpus]\n",
    "vocabulary = set(word for sentence in tokens for word in sentence)\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create one-hot encoded vectors for each word in the vocabulary\n",
    "vocab_size = len(vocabulary)\n",
    "one_hot_encodings = np.eye(vocab_size)\n",
    "\n",
    "# Define the context window size\n",
    "window_size = 2\n",
    "training_pairs = []\n",
    "\n",
    "# Generate training pairs with one-hot encoding\n",
    "for sentence in tokens:\n",
    "    sentence_indices = [word_to_index[word] for word in sentence]\n",
    "    for i, word_index in enumerate(sentence_indices):\n",
    "        for j in range(max(i - window_size, 0), min(i + window_size + 1, len(sentence_indices))):\n",
    "            if i != j:\n",
    "                center_word_vector = one_hot_encodings[word_index]  # One-hot encoded center word vector\n",
    "                context_word_vector = one_hot_encodings[sentence_indices[j]]  # One-hot encoded context word vector\n",
    "                training_pairs.append((center_word_vector, context_word_vector))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def training_code():\n",
    "    # Define the Word2Vec model architecture (Skip-gram)\n",
    "    embedding_size = 10  # Small embedding size for simplicity\n",
    "    W = np.random.rand(vocab_size, embedding_size)  # Input to hidden layer weights (Vocab size x Embedding size)\n",
    "    W_prime = np.random.rand(embedding_size, vocab_size)  # Hidden to output layer weights (Embedding size x Vocab size)\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.01\n",
    "    epochs = 50\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for center_word_vector, context_word_vector in training_pairs:\n",
    "            # Forward pass\n",
    "            h = np.dot(center_word_vector, W)  # Hidden layer output (1 x Embedding size)\n",
    "            u = np.dot(h, W_prime)  # Score vector (1 x Vocab size)\n",
    "            y_pred = np.exp(u) / sum(np.exp(u))  # Softmax output (1 x Vocab size)\n",
    "            # Backward pass\n",
    "            EI = y_pred - context_word_vector  # Error indicator vector (1 x Vocab size)\n",
    "            dW_prime = np.outer(h, EI)  # Gradient for W_prime (Embedding size x Vocab size)\n",
    "            dW = np.outer(center_word_vector, np.dot( W_prime, EI))  # Gradient for W (Vocab size x Embedding size)\n",
    "            \n",
    "            # Update weights\n",
    "            W -= learning_rate * dW\n",
    "            W_prime -= learning_rate * dW_prime\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "# %lprun -f prof_function prof_function()\n",
    "%lprun -f training_code training_code()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
